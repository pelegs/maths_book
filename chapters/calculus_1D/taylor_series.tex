\section{Taylor series}
\subsection{Introduction}
Say we wish to calculate the value of $\cos(x)$ at some non-trivial value, e.g. $x=\frac{\pi}{7}$, or $x=1.0423$. Of course today we can simply use a computer or a calculator - but how do these even calculate such values? Moreover, a lot of these values are non-algebraic, meaning that we can't express them as decimal fractions to an infinite precision. What if for some important calculation we need to know the value of $\eu^{3.153}$ down to 15 digits, but our calculator only shows 10 digits after the period?

We can answer all these questions by using \emph{approximations} instead of precise values. For example, we know values such as $\sin\left(\frac{\pi}{2}\right)$ with infinite precision (it's exactly $1$), but in the case of less trivial values such as the ones discussed above, we can instead settle for an approximation, as long as it is \textit{close enough} to the actual value for our needs. By ``close enough'' we essentially mean \textit{within some given error range}, e.g. in the case of $\eu^{3.153}$ above, 15 digits of precision means that we want that the value we get is no more than $10^{-15}$ away from the actual value. Let us write this mathematically: if we denote $a$ as our approximation of $\eu^{3.153}$, then
\[
  \left| a-\eu^{3.153}\right| \leq 10^{-15}.
\]
More generally, given a value $y$ and and we want to know it to within an \emph{error range} $\Delta$, then for an approximation $a$ to be acceptable, the following must be true:
\begin{equation}
  \left| a-y \right| \leq \Delta.
  \label{eq:error_range}
\end{equation}

A good method to approximate functions would allow us to get as precise as we wish, given that we put enough ``work'' into finding an approximated value, e.g. we could use it to get an approximation of $\eu^{3.153}$ up to $\Delta=10^{-15}$, but we could also use it to get an approximation up to $\Delta=10^{-20}$ or $\Delta=10^{-30}$ or any other value - we will simply have to do more calculations to reach such low error ranges. Generaly, the more precise we want our approximation to be, the more calculation we would need to carry out.

Unlike most real functions, polynomials are relatively easy to calculate for any real value $x$, since we simply need to perform the following operations: addition, subtraction, muliplication and raising by an integer power, all operations that are easy for both humans and computers to perform. Ideally, we would like to use polynomials to approximate all functions, e.g. given the function $f(x) = \cos(x)$ it would be great if we could find some polynomial $P(x)$ of a finite order $n$ for which $P(x)=\cos(x)$. Unfortunately such a polynomial does not exist, nor does such polynomials exist for $\sin(x), \sqrt{x}, \exp(x), a^{x}\ \left(a>0\right)$ or any other of the so-called elementary functions and their compositions (except, of course, polynomials). The reason for this unfortunate reality is rather complicated, but in short it lies in the fact that these functions are \textit{non-algebric}\footnote{technically $\sqrt{x}$ is algebraic, and there are indeed many methods to calculate its values - but none of these methods are as simple as calculating a polynomial\dots except the one we discuss in this section.}.

However, as mentioned before, for each of these functions, we know at least some values with infinite precision. For example, we know that $\cos(0)=1$ and $\cos\left(\frac{1}{2}\pi\right)=0$. From symmetry we thus know that $\cos(\pi)=-1$ and $\cos\left(\frac{3}{4}\pi\right)=0$. Since $\cos(x)$ is periodic, i.e. $\cos\left(x+2\pi k\right)=\cos(x)$ for any $k\in\mathbb{Z}$, we actually know infinitely many values of the function. For $\exp(x)$ we know that $\EU{0}=1$. \autoref{tab:function_values_infinite_precision} lists some known values of common elementary functions.

\begin{table}[htpb]
	\centering
	\caption{Some known values of common elemntary functions. Each such value is known to an infinite precision.}
	\label{tab:function_values_infinite_precision}
	\begin{NiceTabular}{lll}[
			cell-space-limits=3pt, code-before=\rowcolors{1}{\tabcol!15}{\tabcol!10} \rowcolor{\tabcol!50}{1}
		]
		\toprule
		\RowStyle{\bfseries} Function & $x$ & $f(x)$\\
		\midrule
    $\sqrt{x}$ & $4$ & $2$\\
    $\sin(x)$ & $0$ & $0$\\
    $\cos(x)$ & $0$ & $1$\\
    $\tan(x)$ & $0$ & $0$\\
    $\exp(x)$ & $0$ & $1$\\
    $\log(x)$ & $1$ & $0$\\
		\bottomrule
	\end{NiceTabular}
\end{table}

We can use this knowledge to construct a polynomial with $n$ terms, which approximates the function's value for any $x\in\Rs$ to whatever precision we wish, given that a specific condition is met. We will describe this condition later, but first let us use an example function to construct such a polynomial: $\exp(x)$. Since we only know $\EU{0}$ with infinite precision, we can use it as a simple approximation of $\exp(x)$, which we denote $T_{0}(x)$:
\begin{equation}
  T_{0}(x) = 1.
  \label{eq:zero_order_approx_exp}
\end{equation}
(see \autoref{fig:zero_order_approx_exp})

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-2, xmax=4,
      ymin=-1, ymax=17,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1} node[above, pos=0.85]{$T_{0}(x)=1$};
      \foreach \xi in {-3.5, -3, ..., -0.5, 0.5, 1, ..., 3} {
        \edef\temp{\noexpand\draw[thick, dashed] (\xi, 1) -- (\xi, {exp(\xi)});}
          \temp
        }
    \end{axis}
  \end{tikzpicture}
  \caption{The 0th-order approximation $\exp(x)=1$. The dashed lines show $\Delta_{0}(x)$ as $x$ diverges from $0$.}
  \label{fig:zero_order_approx_exp}
\end{figure}

We call $T_{0}(x)$ the \emph{0th-order approximation} of $\exp(x)$. Obviously, this is not a particularly good approximation: it's perhaps ok-ish for values \textit{really} close to $x=0$, but rapidly diverges from the actual value of $\exp(x)$ as we change $x$. More precisely, the error $\Delta_{0}(x)$ is
\begin{equation}
  \Delta_{0}(x) = \left| \exp(x) - T_{0}(x) \right| = \left| \exp(x)-1 \right|,
  \label{eq:exp_err_0}
\end{equation}
i.e. the error essentialy grows as $\exp(x)$ which is\dots not great.

Ok, then perhaps we can add another term to the approximation? After all, our notation pretty much suggests that there are more terms we can use\footnote{more like \textit{screams} it.}. We know that close to a point $a$ a differential function $f(x)$ behaves pretty much like its \textit{derivative} at that point, $f'(a)$. We also know the value of the derivative of $\exp(x)$ at $x=0$, i.e. $\exp'(0)=1$, so we can add a line of slope $m=1$ to our approximation, yielding:
\begin{equation}
  \exp(x) = 1 + x\cdot\exp'\left(0\right) = 1+x.
  \label{eq:first_order_approx_exp}
\end{equation}

Unsurprisingly, we call $T_{1}(x)$ the \emph{1st-order approximation} of $\exp(x)$. Looking at \autoref{fig:first_order_approx_exp} we see that this approximation is better than the previous one, at least for values near $x=0$. In other words, the error $\Delta_{1}$ behaves a bit better than $\Delta_{0}$:
\begin{equation}
  \Delta_{1}(x) = \left| \exp(x) - T_{1}(x) \right| = \left| \exp(x)-x-1 \right|.
  \label{eq:exp_err_1}
\end{equation}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-5, xmax=5,
      ymin=-4, ymax=19,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1+x} node[below, pos=0.9, rotate=27]{$T_{1}(x)$};
    \end{axis}
  \end{tikzpicture}
  \caption{The 1st-order approximation $T_{1}(x)=1+x$.}
  \label{fig:first_order_approx_exp}
\end{figure}

Similarly to $\Delta_{0}(x)$, $\Delta_{1}(x)$ also generally grows as $\exp(x)$. However, closer to $x=0$ it behaves like $\exp(x)-x$, so still not great - but definitely an improvement over $\Delta_{0}(x)$. In practical terms, this means that by using $T_{!}(x)$ we would generally get better approximations than $T_{0}(x)$, at least close to $x=0$. But we can do better!

The next step is of course to add an $x^{2}$ term. The second derivative of $\exp(x)$ is also $\exp(x)$, and at $x=0$ also equal to $1$. This time we will divide the term by $2$ (for know just accept it as is, it would be explained when we formalize the method). Altogether, we get
\begin{equation}
  T_{2}(x) = 1 + x + \frac{x^{2}}{2}.
  \label{eq:second_order_approx_exp}
\end{equation}
(see \autoref{fig:second_order_approx_exp})

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-4, xmax=4,
      ymin=-1, ymax=13,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1+x+x^2/2} node[below, pos=0.7, yshift=-9mm]{$T_{2}(x)$};
    \end{axis}
  \end{tikzpicture}
  \caption{The 2nd-order approximation $T_{2}(x)=1+x+\frac{x^{2}}{2}$.}
  \label{fig:second_order_approx_exp}
\end{figure}

Definitely an improvement, but why stop here? We know all the derivatives of $\exp(x)$ at $x=0$: they are all $1$, since $\exp'(x)=\exp(x)$. We can continue adding power terms, yielding the following general approximation:
\begin{equation}
  T_{n}(x) = 1 + x + \frac{x^{2}}{2} + \frac{x^{3}}{3!} + \frac{x^{4}}{4!} + \cdots + \frac{x^{n}}{n!} = \sum\limits_{k=0}^{n}\frac{x^{k}}{k!}.
  \label{eq:n_order_approx_exp}
\end{equation}
(again, for now you should just accept the coefficients $\frac{1}{n!}$, they would be explained soon)

The higher $n$ is, the more term are in the approximation and it gets more precise (see \autoref{fig:exp_approx}). Of course this means that we need to carry out more calculations, as suggested earlier.

\newcommand{\taylorExp}[1]{
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        graph2d,
        width=7.5cm, height=6cm,
        xmin=-5, xmax=5,
        ymin=-4, ymax=13,
        major grid style={black!10},
        minor grid style={black!5},
        declare function={expTaylor(\k,\x)=\x^\k/factorial(\k);},
        ]
        \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
        \pgfmathtruncatemacro{\n}{#1}
        \addplot[function, xblue, summand=expTaylor] {1+sum(\n,\x)};
        \node[draw=xblue, thick, rounded corners, fill=white, align=left, text=xblue] at (-4,10) {$n=\n$};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}
  \hfill
}

\begin{figure}
  \centering
  \taylorExp{1}
  \taylorExp{2}
  \taylorExp{3}
  \taylorExp{4}
  \taylorExp{5}
  \taylorExp{6}
  \taylorExp{7}
  \taylorExp{8}
  \caption{The function $\exp(x)$ and its approximation $T_{n}(x)$ for several values of $n$.}
  \label{fig:exp_approx}
\end{figure}

This kind of approximation, where we use a polynomial to approximate a real function, is called a \emph{Taylor series} (sometimes also \emph{Taylor expansion}). To calculate the Taylor series of a real function $f(x)$, we must first select a value of $a\in\Rs$ for which we know $f(a)$ with infinite precision (e.g. $x=0$ in the case of $\exp(x)$). The function $f$ must also be infinitely differentiable at $a$, i.e. the $k$-th order derivative $f^{(k)}(a)$ must exist for any $k\in\mathbb{N}$, and we must know its value with inifinite precision. If these conditions are met, then the Taylor series of order $n$ of $f(x)$ at $x=a$ is given by
\begin{equation}
  T_{n}(x) = \sum\limits_{k=0}^{n}\frac{f^{(k)}(a)}{k!}(x-a)^{k}.
  \label{eq:taylor_series}
\end{equation}
We say that a Taylor series as described above is \textit{expanded about} the point $x=a$.

\begin{note}{Maclaurin series}{}
  A Taylor series with $a=0$, i.e. where the series is expanded about $x=0$, is called a \emph{Maclaurin series}.
\end{note}

\begin{example}{Taylor exansion of $\exp(x)$}{}
  Let us verify that using \autoref{eq:taylor_series} indeed yields \autoref{eq:n_order_approx_exp}: since we're using $a=0$, we need to know the value $\exp^{(k)}(0)$ for any $k\in\mathbb{N}$. This is pretty simple: the $k$-th derivative of $\exp(x)$ is $\exp(x)$, and therefore $\exp^{(k)}(0)=1$ for any $k\in\mathbb{N}$. Substituting $a=0$ and $f^{(k)}(x)=1$ to \autoref{eq:taylor_series} gives back \autoref{eq:n_order_approx_exp}.
\end{example}

\begin{example}{Taylor series of $\sin(x)$ and $\cos(x)$}{}
  Now let us calculate the Taylor series of $\sin(x)$ and $\cos(x)$, denoting them as $T^{\text{s}}_{n}(x)$ and $T^{\text{c}}_{n}(x)$, respectively. For both the functions we can use $a=0$ since we know the values of the functions at these points: $\sin(0)=0,\ \cos(0)=1$. The $k$-th order derivative of $\sin(x)$ depends on the value of $k$ modulo $4$, i.e. the derivatives are a repeating sequence of $4$ element:
  \[
    \tikzmark{a}{}\sin(x) \xrightarrow[]{\od{}{x}} \cos(x) \xrightarrow[]{\od{}{x}} -\sin(x) \xrightarrow[]{\od{}{x}} -\cos(x)\ \tikzmark{b}{}
  \]
  \tikz[overlay, remember picture]{\draw[thick, ->](pic cs:b) to[out=-10, in=190, looseness=1]node[midway, below]{$\od{}{x}$}(pic cs:a)}

  \vspace{1em}
  Since each \textbf{even} order derivative of $\sin(x)$ is $\pm\sin(x)$, and $\pm\sin(0)=0$, we can ignore these terms, since they will vanish from the sum. We therefore need to ``run'' the sum only for \emph{odd} values, which we can write as $m=2k+1$. It's important to notice that the values of the derivatives ``jump'' between positive and negative valiues, i.e. $+\cos(0),-\cos(0),+\cos(0),-\cos(0),\dots = +1,-1,+1,-1,\dots$. We therefore set the derivative term to be $(-1)^{k}$. Altogether we get
  \[
    T^{\text{s}}_{n}(x) = x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\cdots\pm\frac{x^{2n+1}}{(2n+1)!} = \sum\limits_{k=0}^{n}\frac{(-1)^{k}}{(2k+1)!}x^{2k+1}.
  \]

  Similarly, the Taylor series for $\cos(x)$ would have only half of its terms non-zero, but the parity is opposite in comparison to $\sin(x)$: this time, the odd terms disappear, and we're left with only the even terms. Therefore we use $m=2k$, and altogether get
  \[
    T^{\text{c}}_{n}(x) = 1-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}-\frac{x^{6}}{6!}+\cdots\pm\frac{x^{2n}}{(2n)!} = \sum\limits_{k=0}^{n}\frac{(-1)^{k}}{(2k)!}x^{2k}.
  \]
\end{example}

\tbw{When reaching the proof that in the limit $n\to\infty$ the Taylor series equals the function, mention how this can be used to prove Euler's theorem}

\subsection{Remainder}
Recall that one of the requirements we put on an approximation of a function $f(x)$ is that it can get \textit{as close} to $f(x)$ as we want, for any point $x=a$ on some interval $(b,c)$ where $b<c\in\mathbb{R}$. This means that given an approximation $T_{n}(x)$ we can write the function to be approximated as
\begin{equation}
  f(x) = T_{n}(x) + R_{n}(x),
  \label{eq:approximation_remainder}
\end{equation}
i.e. that the function to be approximated can be written as the some of out approximation $T_{n}(x)$, and a \emph{remainder} which we can make as small as we want. ``As small as we want'' means that
\begin{equation}
  \lim\limits_{n\to\infty}R_{n}(x) = 0,
  \label{eq:remainder_to_zero}
\end{equation}
or writing this in another way
\begin{equation}
  \lim\limits_{n\to \infty}T_{n}(x)=f(x).
  \label{eq:approximation_precise}
\end{equation}
The last equation shows exactly what we expect of $T_{n}(x)$: as we increase $n$ the approximation gets closer and closer to the function $f(x)$ which we are approximating.

In essence, Taylor series do exactly that for functions which are infinitely differentiable on some interval: at the limit $n\to\infty$ they give back the function itself (\autoref{eq:approximation_precise}). This is called \emph{Taylor's theorem}.
\begin{note}{No proof}{}
  We won't prove Taylor's theorem here since it requires some more formal calculus. Instead, we would just take as granted that it is indeed true.
\end{note}

The convergence of the Taylor series allows us to prove some theorems about several functions, one of which we will use soon. For now though we concentrate on the use of Taylor series as approximations.

Using approximations is always a balance between getting a more precise approximation on one hand, and not doing too much work on the other hand. For example, if we want to use the Taylor series of $\exp(x)$ to calculate $\exp(3.153)$ up to $10^{-15}$, we want to know the minimum value of $n$ such that the remainder $R_{n}(x)$ is smaller then or equal to $10^{-15}$. Any higher $n$ would also give us an approximation within that range, however it will require us to calculate more terms of $T_{n}(x)$. Therefore, we should find a way to estimate $R_{n}(x)$ as a function of $n$, and determine the minimum amount of $n$ needed for an approximation within the desired range.

How do we go about that? Say we have the following:
\begin{itemize}
  \item A function $f(x)$,
  \item a point $x_{0}$ for which we want to know the value $f\left(x_{0}\right)$,
  \item a number of terms $n$ for the Taylor series, and
  \item a point $a$ for which we know precisely the values of $f^{(k)}(a)$ for $k=0,1,2,\dots,n$ and which we use to expand the Taylor series,
\end{itemize}
then the remainder $R_{n}\left(x_{0}\right)$ is a number of the following form:

\begin{equation}
  R_{n}\left(x_{0}\right) = \left| \frac{f^{\left(n+1\right)}(c)}{(n+1)!}\left(x_{0}-a\right)^{n+1} \right|,
  \label{eq:taylor_remainder_bound}
\end{equation}
where $c$ is some point between $a$ and $x_{0}$: if $a>x_{0}$ then $c\in\left[x_{0},a\right]$, and if $a<x_{0}$ then $c\in\left[a,x_{0}\right]$.

This definition might seem a bit much at first, so let us understand it better: according to \autoref{eq:taylor_remainder_bound} the remainder of the approximation of a function $f(x)$ at point $x_{0}$ about $a$ using the Taylor series $T_{n}(x)$ is some number $R_{n}\left(x_{0}\right)$. This number is unknown, otherwise we would know the value we wish to approximate. Instead, we do know that there exist some value $c$ between $a$ and $x_{0}$ such that the remainder $R_{n}\left(x_{0}\right)$ is equal to what shown in \autoref{eq:taylor_remainder_bound}.

(figure?)

In order to estimate the remainder in a usefull way, we can maximize it: if we know the value $c$ for which $R_{n}\left(x_{0}\right)$ is maximal, we know that the actual remainder, which is the error of our estimation, is less than or equal to the remainder.That way we not only have an approximation of $f\left(x_{0}\right)$, but also an estimation of how big the error in approximation could be.

\begin{note}{Two different numbers $c$}{}
  The number $c$ which maximizes $R_{n}\left(x_{0}\right)$ is \textbf{not} the same as the number $c$ for which $T_{n}\left(x_{0}\right) + R_{n}\left(x_{0}\right) = f\left(x_{0}\right)$, otherwise we would have the exact value of $f\left(x_{0}\right)$. Instead, it is the value which \textbf{maximizes} $R_{n}\left(x_{0}\right)$ so we can estimate the error in approximation.
\end{note}

\begin{example}{Estimating the error in approximation}{}
  We will now approximate $\eu$ using $n=6$ terms of the respective Taylor series, and estimate the maximum error of estimation. First we notice that $\eu=exp(1)$, so we would use the following:
  \begin{itemize}
    \item The function $\exp(x)$,
    \item The point $x_{0}=1$ to approximate $\exp(1)=\eu$,
    \item The point $a=0$ for which we know the value $\exp(0)=1$ (which is any order derivative of $\exp(0)$), and
    \item $n=6$ terms for the Taylor series.
  \end{itemize}
  Since in our case $(x-a)^{k}=(1-0)^{k}=1^{k}=1$, and $\exp^{(k)}(0)=\exp(0)=1$ for any $k$, we simply get
  \[
    \eu = \exp(1) = 1 + 1 + \frac{1}{2} + \frac{1}{3!} + \frac{1}{4!} + \frac{1}{5!} + \frac{1}{6!} \approx 2.718056.
  \]

  To estimate the error of the approximation, we should find the maximal value of
  \[
    R_{6}(1) = \left| \frac{\exp(c)}{6!} \right|.
  \]
  in the interval $[0,1]$. Since $\exp(x)>0$, we can drop the absolute value since the expression must be positive. And since $\exp(x)$ always increases so does $R_{6}$ as a function of $c$, and we therefore know that its maximum value is when $c=1$.

  This gives a maximum remainder $R = \frac{\exp(1)}{6!}$. However, to use this we still need the value of $\exp(1)$ which is exactly what we are trying to caluclate\dots instead, we will use a known inequality: $\eu<3$, and therefore
  \[
    R<\frac{3}{6!}=\frac{3}{720}=0.00416666\dots<0.004167.
  \]
\end{example}

\begin{example}{Another approximation and estimation of its error}{}
  Since $\tan\left(\frac{\pi}{4}\right)=1$, we can use $\arctan(x)$ to yield $\pi$: $\arctan(1)=\frac{\pi}{4}$ and therefore $4\times\arctan(1)=\pi$. To approximate $\arctan(x)$ at $x_{0}=1$ we would use the following Taylor series:
  \[
    T_{n}(x) = x - \frac{x^{3}}{3} + \frac{x^{5}}{5} - \frac{x^{7}}{7} + \frac{x^{9}}{9} - \cdots + (-1)^{n}\frac{x^{2n+1}}{2n+1}.
  \]
  (you should verify for yourself that this is indeed the Taylor series of $\arctan(x)$ expanded about $a=0$)

  Plugging $x=1$ and $n=9$ we get
  \[
    T_{n}\left(1\right) = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} = \frac{105-35+21-15+11}{105} = \frac{87}{105}.
  \]
  Multiplying by $4$ gives us
  \[
    4\frac{87}{105} \approx 3.3142857,
  \]
  which is an ok-ish approximation of $\pi$, definitely for such a low effort. Let us now estimate the maximum error of the approximation: plugging $x_{0}=1$, $a=0$, $f(x)=\arctan(x)$ and $n=9$ into \autoref{eq:taylor_remainder_bound} we get
  \[
    R = \left| \frac{\arctan^{(10)}(c)}{10!} \right|.
  \]
  (since $x_{0}-a$ = $1-0$ = 1)

  The $10$-th derivative of $\arctan(x)$ is the following monstrosity\footnote{calculated using \textit{sympy}, because doing this by hand is pure masochism.}:
  \[
    \arctan^{(10)}(x) = \frac{725760 x \left(- \frac{256 x^{8}}{\left(x^{2} + 1\right)^{4}} + \frac{512 x^{6}}{\left(x^{ 2} + 1\right)^{3}} - \frac{336 x^{4}}{\left(x^{2} + 1\right)^{2}} + \frac{80 x^{2}}{x^{2} + 1} - 5\right)}{ \left(x^{2} + 1\right)^{6}}.
  \]

  Plugging back into $R$ (recall the absolute value) yields
  \[
    R = \left| \frac{\arctan^{(10)}(c)}{10!} \right|.
  \]
  It's not easy to find an estimation for the maximum value of $R$ on $[0,1]$, however by using a computer\footnote{yeah, that's kind of cheating\ldots} we can say for sure that it is smaller than $\frac{1.55\times10^{7}}{10!}=$
\end{example}

\tbw{Hmmm\ldots the 2nd example needs some work.}
\tbw{How to choose $n$ when max error is given.}

Before continuing to discussing the Taylor series coefficients, let us show a use of Taylor's theorem to prove Eurler's formula.
\begin{proof}{Euler's formula}{}
  Euler's formula (\autoref{eq:euler_formula}) states that
  \[
    \eu^{\iu x} = \cos(x) + \iu\sin(x).
  \]
  We can express both $\cos(x)$ and $\sin(x)$ using infinite Taylor series:
  \begin{itemize}
    \item $\cos(x) = 1-\frac{x^{2}}{2}+\frac{x^{4}}{4!}-\frac{x^{6}}{6!}+\cdots$.
    \item $\sin(x) = x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\cdots$.
  \end{itemize}
  Therefore, $\cos(x)+\iu\sin(x)$ can be written as
  \begin{align*}
    \cos(x) + \iu\sin(x) &= \left(1-\frac{x^{2}}{2}+\frac{x^{4}}{4!}-\frac{x^{6}}{6!}+\cdots\right) + \iu\left(x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\cdots\right)\\
                         &= 1+\iu x - \frac{x^{2}}{2} -\iu\frac{x^{3}}{3!} + \frac{x^{4}}{4!} + \iu\frac{x^{5}}{5!} - \frac{x^{6}}{6!} - \iu\frac{x^{7}}{7!} + \cdots\\
                         &= 1 +\iu x + \frac{(\iu x)^{2}}{2} + \frac{\left(\iu x\right)^{3}}{3!} + \frac{(\iu x)^{4}}{4!} + \frac{\left(\iu x\right)^{5}}{5!} + \frac{(\iu x)^{6}}{6!} + \frac{(\iu x)^{7}}{7!} + \cdots\\
                         &= \eu^{\iu x}.
  \end{align*}

  The last equation is true since the powers of $\iu$ are a repeating sequence of period $4$:
  
  \centering
  \begin{tabular}{lllllllll}
    $\iu^{0}$ & $\iu^{1}$ & $\iu^{2}$ & $\iu^{3}$ & $\iu^{4}$ & $\iu^{5}$ & $\iu^{6}$ & $\iu^{7}$ & $\dots$\\
    $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$ & \dots\\
    $1$ & $\iu$ & $-1$ & $-\iu$ & $1$ & $\iu$ & $-1$ & $-\iu$ & $\dots$\\
  \end{tabular}
\end{proof}

\subsection{Coefficients}
Up until now we simply took the coefficients $\frac{1}{n!}$ of the Taylor series for granted. Let us now understand how these coefficient come about.

As stated before, in the limit $n\to\infty$ the Taylor series of a function $f(x)$ equals $f(x)$ precisely (given all conditions for $f(x)$ are met):
\begin{align}
  f(x) & = T(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2}(x-a)^{2} + \frac{f'''(a)}{3!}(x-a)^{3} + \dots\\
       &= \sum\limits_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-a)^{k}.
  \label{eq:taylor_precise}
\end{align}
Let us now imagine we don't know the coefficients $\frac{1}{k!}$ and instead call them $\lambda_{0}, \lambda_{1}, \lambda_{2}, \dots$:
\begin{equation}
  f(x) = \sum\limits_{k=0}^{\infty}\lambda_{k}f^{(k)}(a)(x-a)^{k}.
  \label{eq:taylor_unknown_coeffs}
\end{equation}

Deriving both sides of \autoref{eq:taylor_unknown_coeffs} yields
\begin{align}
  f'(x) &= \sum\limits_{k=0}^{\infty}\lambda_{k}f^{(k)}(a)\cdot k(x-a)^{k-1}\\
        &= \lambda_{1}f'(a) + 2\lambda_{2}f''(a)(x-a) + 3\lambda_{3}f'''(a)(x-a)^{2} + \dots
  \label{eq:taylor_unknown_coeffs_1st_derivative}
\end{align}

Subtituting $x=a$ we get $f'(a)$ on the lhs and on the rhs all the terms with the an expression of the form $(x-a)^{k-1}$ go to zero. We are thus left with
\[
  f'(a) = \lambda_{1}f'(a),
\]
or simply
\begin{equation}
  \lambda_{1} = 1.
  \label{eq:Taylor_1st_coefficient}
\end{equation}

Deriving \autoref{eq:taylor_unknown_coeffs} again, subtituting $\lambda_{1}=1$ we get
\[
  f''(x) = 2\lambda_{2}f''(a) + 3\cdot2\lambda_{3}f'''(a)(x-a) + 4\cdot3\lambda_{4}f^{(4)}(a)(x-a)^{3} + \dots
\]
and subtituting $x=a$ again makes all the terms of the form $(x-a)^{k-2}$ to go to zero, giving us
\[
  f''(a) = 2\lambda_{2}f''(a),
\]
i.e.
\begin{equation}
  \lambda_{2} = \frac{1}{2}.
  \label{eq:Taylor_2nd_coefficient}
\end{equation}

We can repeat this process again:
\[
  f'''(x) = 3\cdot2\lambda_{3}f'''(a) + 4\cdot3\cdot2f^{(4)}(a)(x-a) + 5\cdot4\cdot3f^{(5)}(a)(x-a)^{2} + \dots
\]
and subtituting $x=a$ gives
\[
  f'''(a) = 3!\lambda_{3}f'''(a),
\]
i.e.
\begin{equation}
  \lambda_{3} = \frac{1}{3!}.
  \label{eq:Taylor_3rd_coefficient}
\end{equation}

We can see that in general, the $m$-th derivative of \autoref{eq:taylor_unknown_coeffs} would be
\begin{equation}
  f^{(m)}(x) = \sum\limits_{k=m}^{\infty}k!\lambda_{k}f^{(k)}(a)(x-a)^{k-m},
  \label{eq:Taylor_nth_coefficient}
\end{equation}
and subtituting $x=a$ would take all terms with index $k>m$ to zero, leaving us with
\[
  f^{(m)}(a) = m!\lambda_{m}f^{(m)}(a),
\]
and thus
\begin{equation}
  \lambda_{m} = \frac{1}{m!}.
  \label{eq:Taylor_mth_coefficient}
\end{equation}
