\section{The Bra-Ket notation}
\tbw{this section needs some rework regarding dual vectors}

In this section we introduce a special vector notation widely used in physics: the \emph{Bra-ket notation}, also known as \emph{Dirac's notation}. This notation helps simplify many aspects of linear algebra, and make its use more streamlined.

\begin{note}{Importance of this section}{}
	A person can have a pretty good grasp of linear algebra without ever learning about the bra-ket notation. This section, while interesting and providing a useful tool in working with linear algebra - is not obligatory, especially to people who do not intend to ever learn topics such as quantum physics, relativity theory, statistical mechanics, etc. It is however recommended even for those readers.
\end{note}

\subsection{Definition}
Up until now we presented the theory of linear algebra based on real numbers: all of the vectors we used were real vectors, i.e. of the form $\vec{v}\in\Rs[n]$, where $n\in\mathbb{N}$. All of the matrices used were also made up of real components - and so were of course the scalars themselves, which we defined simply as real numbers.

However, it is apparently useful in many cases to use linear algebra in the context of complex numbers: instead of working with spaces of the form $\Rs[n]$ we can use spaces of the form $\Cs[n]$, e.g. a vector in $\Cs[3]$ can be the following:
\[
	\vec{v} = \colvec{1-i;2+3i;\pi+\sqrt{2}i}.
\]

When using complex numbers instead of real numbers, a small change must be made to the way we conceptualize row vs. column vectors. Before we said that essentially both forms can be used interchangeably without affecting the outcome. However now we define row vectors a bit differently: given the column vector
\begin{equation}
	\vec{v} = \colvec{v_{1};v_{2};\vdots;v_{n}},
	\label{eq:column_vector_complex}
\end{equation}
we can get its row form by transposing it, i.e. we look at $\vec{v}^{\top}$. However, when doing this we must change all the components of $\vec{v}$ to their respective complex conjugates, i.e.
\begin{equation}
	\vec{v}^{\top} = \colvec{v_{1};v_{2};\vdots;v_{n}}^{\top} = \rowvec{\conj{v_{1}};\conj{v_{2}};\dots;\conj{v_{n}}}.
	\label{eq:row_vector_complex}
\end{equation}


\begin{equation}
	\vec{v}^{\top} = \colvec{v_{1};v_{2};\vdots;v_{n}}^{\top} = \rowvec{\sconj{v_{1}};\sconj{v_{2}};\dots;\sconj{v_{n}}}.
	\label{eq:row_vector_complex_star_notation}
\end{equation}
  To be more succinct we can adopt some changes in notation widely used in physics. 
\begin{itemize}
	\item The complex conjugate of the number $z\in\Cs$ is changed to $\sconj{C}$.
	\item The star notation is also used for transpose (this is normaly called \emph{conjugate transpose}, and is sometimes denoted by $u^{\dagger}$).
	\item The arrow is dropped from the vector notation.
\end{itemize}
Applying these changes, \autoref{eq:row_vector_complex_star_notation} has the form
\begin{equation}
	\sconj{v} = \sconj{\colvec{v_{1};v_{2};\vdots;v_{n}}} = \rowvec{\sconj{v_{1}};\sconj{v_{2}};\dots;\sconj{v_{n}}}.
	\label{eq:row_vector_complex_star_notation_transpose}
\end{equation}

\begin{example}{Bra/Row vectors}{}
	The bra form of the vector
	\[
		v = \colvec{1+2i;3-i;\sqrt{2}+5i;4;-3i}
	\]
	is
	\[
		\sconj{v} = \sconj{\colvec{1+2i;3-i;\sqrt{2}+5i;4;-3i}} = \rowvec{1-2i;3+i;\sqrt{2}-5i;4;3i}.
	\]
\end{example}

Next, we make sure that the scalar product between any two vectors $u,v$ is such that the left vector is a row vector, and the right vector is a column vector, i.e. given $u,v\in\Cs[n]$ such that
\[
	u = \colvec{u_{1};u_{2};\vdots;u_{n}} \quad v = \colvec{v_{1};v_{2};\vdots;v_{n}},
\]
their scalar product is
\begin{equation}
	u\cdot v = \sconj{u}\cdot v = \rowvec{\sconj{u_{1}};\sconj{u_{2}};\dots;\sconj{u_{n}}} \cdot \colvec{v_{1};v_{2};\vdots;v_{n}} = \sconj{u_{1}}v_{1} + \sconj{u_{2}}v_{2} + \cdots + \sconj{u_{n}}v_{n}.
	\label{eq:scalar_product_complex_full}
\end{equation}

Recall that a common notation for the scalar product of two vectors uses triangular brakets, i.e.
\[
	u\cdot v = \innerproduct{u}{v}.
\]
We can use \autoref{eq:scalar_product_complex_full} and ``separate'' this product into two parts: a \emph{bra} $\bra{u}$ and a \emph{ket} $\ket{v}$, define as
\begin{align}
	\bra{u} &= \rowvec{\sconj{u_{1}};\sconj{u_{2}};\dots;\sconj{u_{n}}},\nonumber\\
	\ket{v} &= \colvec{v_{1};v_{2};\vdots;v_{n}}.
	\label{eq:bra_ket_def}
\end{align}

\subsection{Norm and products}
The norm of a vector $v$ can be calculated by taking the square root of its scalar product with itself (\autoref{eq:norm from scalar product}). Using the bra-ket notation this becomes
\begin{equation}
	\norm{v} = \sqrt{\innerproduct{v}}.
	\label{eq:norm_braket}
\end{equation}

Let us write the properties of the scalar product adjusted to the bra-ket notation:
\begin{descitemize}
	\item[Non-negative norm] for any vector $v\in\Cs[n],\ \innerproduct{v}\geq0$.
	\item[Uniqeness of zero] if $\innerproduct{v}=0$, then $v=0$.
	\item[Conjugate commutativity] for any two vectors $u,v\in\Cs[n],\ \innerproduct{u}{v}=\sconj{\innerproduct{v}{u}}$.
	\item[Distributivity] Given three vectors $u,v,w\in\Cs[n]$ and two scalars $\alpha,\beta\in\Cs$,
		\[
			\bra{u} \left( \alpha\ket{v}+\beta\ket{w} \right) = \alpha\innerproduct{u}{v} + \beta\innerproduct{u}{w}.
		\]
\end{descitemize}

\begin{note}{Hilbert spaces}
 Generally speaking, any vector space that is ``equipped'' with a norm complying with these properties is called a \emph{Hilbert space}. We will discuss such spaces in more details later in the book.
\end{note}

There is an interesting way one can interpret the scalar product: instead of as an operation acting on two vectors, we can view a bra as an operator acting on a ket and returning a scalar. Mathematically this is written as
\begin{equation}
	\bra{\bigcirc}: \Cs[n]\to\Cs.
	\label{eq:bra_as_operator}
\end{equation}
(the empty circle signifies that that the symbol representing the bra is placed inside the bra symbol)

\tbw{this is the dual space of $\Cs[n]$, etc.}

Another product that is easily defined usin the bra-ket notation is the \emph{exterior product} of two vectors (recall that the scalar product is also called the \textit{inner product}). The exterior product arises when we multiply two vectors in the opposite order compared to the scalar product, i.e. instead of $\innerproduct{u}{v}$ we calculate $\outerproduct{u}{v}$:
\begin{equation}
	\outerproduct{u}{v} = \colvec{\Ui{1};\Ui{2};\vdots;\Ui{n}}\rowvec{\sconj{\Vj{1}};\sconj{\Vj{2}};\dots;\sconj{\Vj{n}}} =
	\begin{bNiceMatrix}
		\Ui{1}\sconj{\Vj{1}} & \Ui{1}\sconj{\Vj{2}} & \dots & \Ui{1}\sconj{\Vj{n}}\\
		\Ui{2}\sconj{\Vj{1}} & \Ui{2}\sconj{\Vj{2}} & \dots & \Ui{2}\sconj{\Vj{n}}\\
		\vdots & \vdots & \Ddots & \vdots\\
		\Ui{n}\sconj{\Vj{1}} & \Ui{n}\sconj{\Vj{2}} & \dots & \Ui{n}\sconj{\Vj{n}}\\
	\end{bNiceMatrix},
	\label{eq:outer_product}
\end{equation}
i.e. the result of the outer product is a matrix, in which any element $\Ma{i}{j}$ is equal to
\begin{equation}
	\Ma{i}{j} = \Ui{i}\sconj{\Vj{j}}.
	\label{eq:}
\end{equation}

\begin{note}{Bra-ket vs. ket-bra}{}
	Using the bra-ket notation, the names for the inner and outer products make sense: in the inner product the vectors stay inside the brackets, while in the outer product they are outside of it. This is just a small demonstration for the ``power'' of the bra-ket notation in simplifying mathematical expressions.
\end{note}

\begin{example}{Inner and outer products}{}
	Given the two kets
	\[
		\ket{u}=\colvec{1+\iu;3},\ \ket{v}=\colvec{-2+3\iu;5-\iu},
	\]
	let us calculate the following:
	\[
		\innerproduct{u}{v},\ \innerproduct{v}{u},\ \outerproduct{u}{v},\ \outerproduct{v}{u}.
	\]

	We start by writing $\bra{u}$ and $\bra{v}$:
	\[
		\bra{u}=\rowvec{1-\iu;3},\ \bra{v}=\rowvec{-2-3\iu;5+\iu}.
	\]
	Then, the 4 requested products are easy to calculate:
	\begin{align*}
		\innerproduct{u}{v} &= (1-\iu)(-2+3\iu) + 3(5-\iu) = -2+3\iu+2\iu+3+15-3\iu = 16+2\iu.\\
		\innerproduct{v}{u} &= (1+\iu)(-2-3\iu) + 3(5+\iu) = -2-3\iu-2\iu+3+15+3\iu = 16-2\iu = \sconj{\innerproduct{u}{v}}.\\[2em]
		\outerproduct{u}{v} &=
			\begin{bmatrix}
				(1+\iu)(-2-3\iu) & (1+\iu)(5+\iu)\\
				3(-2-3\iu) & 3(5+\iu)
			\end{bmatrix}
			=
			\begin{bmatrix}
				1-5\iu & 4+6\iu\\
				-6-9\iu & 15+3\iu
			\end{bmatrix}.\\
		\outerproduct{v}{u} &=
			\begin{bmatrix}
				(-2+3\iu)(1-\iu) & 3(-2+3\iu)\\
				(5-\iu)(1-\iu) & 3(5-\iu)
			\end{bmatrix}
			=
			\begin{bmatrix}
				1+5\iu & -6+9\iu\\
				4-6\iu & 15-3\iu
			\end{bmatrix}.
	\end{align*}
\end{example}

\subsection{Scalar multiplication and addition of vectors}
Scaling a vector in the bra-ket notation is represented by simply putting the scalar on the left or right of the vector, i.e. given a ket
\[
	\ket{u}=\colvec{u_{1};u_{2};\vdots;u_{n}},
\]
its scaled version by the scalar $\alpha\in\Cs$ is
\begin{equation}
	\alpha\ket{u} = \ket{u}\alpha = \colvec{\alpha u_{1};\alpha u_{2};\vdots;\alpha u_{n}}.
	\label{eq:scaling_ket}
\end{equation}
The bra version is essentialy the same:
\begin{equation}
	\alpha\bra{u} = \bra{u}\alpha = \rowvec{\alpha \sconj{u_{1}};\sconj{\alpha u_{2}};\dots;\sconj{\alpha u_{n}}}.
	\label{eq:scaling_bra}
\end{equation}
Normally, scaling a ket is written with the scalar on the left, and scaling a bra is written with scalar on the right. However, in some instances the other forms are used (we will see such case soon).

Adding two kets or two bras is also quote simple:
\begin{align}
	\ket{u}+\ket{v} &= \ket{u+v} = \colvec{u_{1}+v_{1};u_{2}+v_{2};\vdots;u_{n}+v_{n}},\nonumber\\
	\bra{u}+\bra{v} &= \bra{u+v} = \rowvec{\sconj{\left(u_{1}+v_{1}\right)};\sconj{\left(u_{2}+v_{2}\right)};\dots;\sconj{\left(u_{n}+v_{n}\right)}}.
	\label{eq:addition_bra-ket}
\end{align}

\begin{note}{Addition of a bra and a ket}{}
	Of course, a bra and a ket cannot be added together.
\end{note}

\subsection{Linear combinations and basis sets}
Linear combinations are easily represented using the bra-ket notation: let $\alpha_{1},\alpha_{2},\dots,\alpha_{n}$ be complex numbers, and $\ket{v_{1}},\ket{v_{2}},\dots,\ket{v_{n}}$ kets in $\Cs[n]$. Then
\begin{equation}
	\ket{w} = \alpha_{1}\ket{v_{1}} + \alpha_{2}\ket{v_{2}} + \cdots + \alpha_{n}\ket{v_{n}} = \sum\limits_{i=1}^{n}\alpha_{i}\ket{v_{i}}
	\label{eq:bra-ket_linear_combination}
\end{equation}
is of course itself a vector in $\Cs[n]$. If the set $B=\left\{\ket{b_{1}},\ket{b_{2}},\dots,\ket{b_{n}}\right\}$ is a basis set of $\Cs[n]$ then any vector $\vec{v}\in\Cs[n]$ can be written as a linear combination of the vectors in $B$, i.e.
\begin{equation}
	\vec{w} = \sum\limits_{i=1}^{n}\beta_{i}\ket{b_{i}},
	\label{eq:bra-ket_spanning_vectors}
\end{equation}
such that not all $\beta_{i}=0$ (i.e. at least one of them is non-zero). If in addition
\begin{equation}
	\innerproduct{b_{i}}{b_{j}} = \delta_{ij},
	\label{eq:bra-ket_orthonormal_basis}
\end{equation}
the basis set is orthonormal: each vector has unit norm (since $\innerproduct{b_{i}}=\delta_{ii}=1$), and they are all orthogonal to each other (since for $i\neq j,\ \innerproduct{b_{i}}{b_{j}}=\delta_{i\neq j} = 0$).

Sometimes, and depending on the context, basis vectors are written as their indeces only inside a bra or a ket. This is common for example with the standard basis vectors, e.g. in $\Cs[3]$
\begin{equation}
	\hat{x}=\ket{1},\ \hat{y}=\ket{2},\ \hat{z}=\ket{3}.
	\label{eq:std_basis_vectors_bra-ket}
\end{equation}
and more generaly
\begin{equation}
	\eb{i}=\ket{i}.
	\label{eq:i-th_std_basis_vector_bra-ket}
\end{equation}

Let us now take a general ket in $\ket{v}\in\Cs[n]$ and write it as a linear combination of some orthonormal basis set $B$:
\begin{equation}
	\ket{v} = \sum\limits_{i=1}^{n}\alpha_{i}\ket{b_{i}}.
	\label{eq:}
\end{equation}
We now choose one of the basis vectors and write it in its bra form: $\bra{b_{j}}$, where $j\in\left\{1,2,\dots,n\right\}$. We can easily write the inner product of $\bra{b_{j}}$ with $\ket{v}$:
\begin{align}
	\innerproduct{b_{j}}{v} &= \bra{b_{j}}\sum\limits_{i=1}^{n}\alpha_{i}\ket{b_{i}}\nonumber\\
	&= \bra{b_{j}}\alpha_{1}\ket{b_{1}} + \bra{b_{j}}\alpha_{2}\ket{b_{2}} + \dots + \bra{b_{j}}\alpha_{n}\ket{b_{n}}.
	\label{eq:}
\end{align}
Since $\alpha_{i}$ are scalars, we can move each one of them to the left of $\bra{b_{j}}$, yielding
\begin{equation}
	\innerproduct{b_{j}}{v} = \alpha_{1}\innerproduct{b_{j}}{b_{1}} + \alpha_{2}\innerproduct{b_{j}}{b_{2}} + \cdots + \alpha_{n}\innerproduct{b_{j}}{b_{n}}.
	\label{eq:}
\end{equation}
Since the basis vectors are all orthonormal, $\innerproduct{b_{j}}{b_{i}}=\delta_{ji}$, or zero for all $i$ except when $i=j$, in which case the product equals $1$. Thus all the terms where $i\neq j$ disappear, and we are left with a single term:
\begin{align}
	\innerproduct{b_{j}}{v} = \alpha_{j}\innerproduct{b_{j}} = \alpha_{j}.
	\label{eq:isolating_scalar_coeffs}
\end{align}
This shows is that we can calculate the coefficient $\alpha_{j}$ for any $j\in\left\{1,2,\dots,n\right\}$ by simply taking the dot product of the $j$-th basis vector with $\ket{u}$. This is very easy compared to using the vector notation we used so far.

We can now start again with spanning $\ket{u}$ using $B$, but this time we write the scalar coefficients on the right side:
\begin{equation}
	\ket{u} = \sum\limits_{i=1}^{n}\ket{b_{i}}\alpha_{i}.
	\label{eq:coeffs_right_side}
\end{equation}
Subtituting \autoref{eq:isolating_scalar_coeffs} into \autoref{eq:coeffs_right_side} we get
\begin{equation}
	\ket{u} = \sum\limits_{i=1}^{n}\ket{b_{i}}\innerproduct{b_{i}}{u}.
	\label{eq:ket_as_sum_of_outer_products}
\end{equation}
Since $\ket{u}$ is found in all the terms of the sum, we can put parenthesis around the summation excluding $\ket{u}$:
\begin{equation}
	\ket{u} = \left(\sum\limits_{i=1}^{n}\ket{b_{i}}\bra{b_{i}}\right)\ket{u}.
	\label{eq:ket_as_sum_of_outer_products_parenthesis}
\end{equation}

Recall that the outer product return a matrix. Thus, \autoref{eq:ket_as_sum_of_outer_products_parenthesis} tells us that the matrix $A=\sum\limits_{i=1}^{n}\ket{b_{i}}\bra{b_{i}}$ is a matrix that doesn't change any vector $\ket{u}$ (remember that there is nothing special about $\ket{u}$, we didn't even write it explicitly). This matrix is of course the identity matrix, i.e.
\begin{equation}
	I_{n}=\sum\limits_{i=1}^{n}\ket{b_{i}}\bra{b_{i}}.
	\label{eq:the_completeness_relation}
\end{equation}
This result is called the \emph{completeness relation}. It essentially tells us that given an orthonormal basis set, the sum of the outer product of each basis vector with itself is the identity matrix. For example, given the standard basis set in $\Cs[3]$,
\begin{equation}
	\outerproduct{1}{1} + \outerproduct{2}{2} + \outerproduct{3}{3} = I_{3}.
	\label{eq:completeness_relation_std_basis_C3}
\end{equation}

\begin{example}{The completeness relation in $\bm{\Cs[2]}$}{}
	As we saw in \autoref{example:orthobasis}, the following basis set is orthonormal:
	\[
		\ket{1}=\frac{1}{\sqrt{2}}\colvec{1;1},\ \ket{2}=\frac{1}{\sqrt{2}}\colvec{1;-1}.
	\]

	The outer products of each of the two vectors with itself are
	\begin{align*}
		\outerproduct{1}{1} &=
		\frac{1}{2}
		\begin{bmatrix}
			1 & 1\\
			1 & 1
		\end{bmatrix},\\
		\outerproduct{2}{2} &=
		\frac{1}{2}
		\begin{bmatrix}
			1 & -1\\
			-1 & 1
		\end{bmatrix}.
	\end{align*}

	Thus,
	\[
		\outerproduct{1}{1}+\outerproduct{2}{2} = \frac{1}{2}\left(\begin{bmatrix} 1&1 \\ 1&1 \end{bmatrix} + \begin{bmatrix} 1&-1 \\ -1&1 \end{bmatrix}\right) = \frac{1}{2}\begin{bmatrix} 2&0 \\ 0&2 \end{bmatrix} = \begin{bmatrix} 1&0 \\ 0&1 \end{bmatrix} = I_{2}.
	\]
\end{example}

\tbw{change of basis, matries inside brakets, eigenvectors, etc.}
